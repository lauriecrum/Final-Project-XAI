{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQame4x3u_vl",
        "outputId": "c467e8bd-d637-47c8-ee60-b479399602dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token Attention Weights:\n",
            "Token: [CLS], Attention Weight: 0.0260\n",
            "Token: we, Attention Weight: 0.0307\n",
            "Token: attend, Attention Weight: 0.0275\n",
            "Token: in, Attention Weight: 0.0185\n",
            "Token: the, Attention Weight: 0.0153\n",
            "Token: class, Attention Weight: 0.0276\n",
            "Token: just, Attention Weight: 0.0282\n",
            "Token: for, Attention Weight: 0.0207\n",
            "Token: listening, Attention Weight: 0.0207\n",
            "Token: teachers, Attention Weight: 0.0281\n",
            "Token: reading, Attention Weight: 0.0234\n",
            "Token: on, Attention Weight: 0.0182\n",
            "Token: slide, Attention Weight: 0.0261\n",
            "Token: ., Attention Weight: 0.0298\n",
            "Token: just, Attention Weight: 0.0430\n",
            "Token: non, Attention Weight: 0.0355\n",
            "Token: ##sen, Attention Weight: 0.0224\n",
            "Token: ##ce, Attention Weight: 0.0246\n",
            "Token: [SEP], Attention Weight: 0.5339\n",
            "\n",
            "Sentiment Prediction: NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Attention rollout function\n",
        "def attention_rollout(attentions):\n",
        "    rollout = torch.eye(attentions[0].size(-1)).to(attentions[0].device)\n",
        "    for attention in attentions:\n",
        "        attention_heads_fused = attention.mean(dim=1)\n",
        "        attention_heads_fused += torch.eye(attention_heads_fused.size(-1)).to(attention_heads_fused.device)\n",
        "        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True)\n",
        "        rollout = torch.matmul(rollout, attention_heads_fused)\n",
        "    return rollout\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sentiment_analysis.csv\")\n",
        "text = df[\"text\"][3]\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, output_attentions=True)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize and run model\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions\n",
        "    logits = outputs.logits\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Simplify prediction to binary (positive/negative)\n",
        "predicted_class = torch.argmax(probs, dim=1).item()\n",
        "binary_sentiment = \"POSITIVE\" if predicted_class >= 3 else \"NEGATIVE\"\n",
        "\n",
        "# Get tokens and attention rollout\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "rollout = attention_rollout(attentions)[0]\n",
        "cls_attention = rollout[0]  # Attention from [CLS] token\n",
        "\n",
        "# Print results\n",
        "print(\"\\nToken Attention Weights:\")\n",
        "for token, weight in zip(tokens, cls_attention):\n",
        "    print(f\"Token: {token}, Attention Weight: {weight.item():.4f}\")\n",
        "\n",
        "print(f\"\\nSentiment Prediction: {binary_sentiment}\")\n"
      ]
    }
  ]
}